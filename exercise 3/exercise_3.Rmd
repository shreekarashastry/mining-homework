---
title: "ECO395M: Exercise 3"
author: "Steven Kim and Shreekara Shastry"
date: ""
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(knitr)
library(rsample)
library(pdp)
library(rpart)
library(rpart.plot)
library(gbm)
library(randomForest)
library(foreach)
library(ggmap)
library(caret)
```

## What causes what?

1. You can't just get data from a few different cities and run the regressions of “Crime” on “Police” because cities have incentives to hire more cops when there is an increased number of crimes. Because of this, it would look like “Crime” is positively correlated to “Police” when there is no reason to believe there is a causal relationship.

How were the researchers from UPenn able to isolate this effect? Briefly describe their approach and discuss their result in the “Table 2” below, from the researchers' paper.

2. The researchers from UPenn were able to isolate this effect because the District of Columbia had the policy where they increased  “Police” when there is an increased risk of terrorism, which is believed to be unrelated to the street crime rates. The result in the “Table 2” below says that there is a statistically significant negative relationship between the “High Alert” and “Crime”, implying that the increased number of cops because of the possible terrorist risk decreased the crime rates. This holds true even after controlling for the ridership of Metro.

3. They had to control for Metro ridership because, if “Crime” decreased because there were less people on the streets, that would not neccesarily mean the rate of crime decreasing because of the increased number of cops. This would be of concern if people stayed home because of the terrorist alert. However, it did not turn out to be true. They were trying to capture the effect of the decrease of normal human activity in the city on the number of crime incidents.

4. The model being estimated here is the effect of “High Alert”, controlled for “Midday Ridership”, by districts (if it is district 1 or not). The conclusion is that the effect of “High Alert” is only significant in the first police district area.


## Tree modeling: dengue cases

```{r}
dengue = read_csv('dengue.csv') %>% drop_na()

dengue$city = dengue$city %>% factor()
dengue$season = dengue$season %>% factor()

set.seed(98236)
dengue_split = initial_split(dengue, 0.8)
dengue_train = training(dengue_split)
dengue_test = testing(dengue_split)

#CART
trctrl = trainControl(method = "cv", number = 5, savePredictions=TRUE)
dengue_tree1 = train(total_cases ~., data = dengue_train, method = "rpart", trControl=trctrl, 
                     tuneLength = 1, control = rpart.control(cp = 0.002, minsplit=30))
dengue_tree2 = train(total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt,
      data = dengue_train, method = "rpart", trControl=trctrl, tuneLength = 1, 
      control = rpart.control(cp = 0.002, minsplit=30))
dengue_tree3 = train(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt,
      data = dengue_train, method = "rpart", trControl=trctrl, tuneLength = 1,
      control = rpart.control(cp = 0.002, minsplit=30))

dengue_tree_rmse = data.frame(
dengue_tree1$results$RMSE,
dengue_tree2$results$RMSE,
dengue_tree3$results$RMSE
)
colnames(dengue_tree_rmse) = c("Tree 1", "Tree 2", "Tree 3")
rownames(dengue_tree_rmse) = "RMSE"
kable(dengue_tree_rmse)

# CART, with all the training data
dengue.tree = rpart(total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt, 
                    data=dengue_train,
                    control = rpart.control(cp = 0.002, minsplit=30))

prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}

dengue.prune = prune_1se(dengue.tree)
```

First, I split the data into training and testing sets. Then, I wanted to choose the best CART model. For CART, we choose the second model with the specification of `total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt` as it has the lowest in-sample cross validated rMSE. Then, I trained again the chosen model with all the training data and pruned the tree.

```{r}
# Random Forest

trctrl = trainControl(method = "cv", number = 5, savePredictions=TRUE)
dengue_rf1 = train(total_cases ~., data = dengue_train, method = "rf", trControl=trctrl,
                   tuneLength = 1, importance = TRUE, na.action=na.omit)
dengue_rf2 = train(total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt,
      data = dengue_train, method = "rf", trControl=trctrl,
       tuneLength = 1, importance = TRUE, na.action=na.omit)
dengue_rf3 = train(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt,
      data = dengue_train, method = "rf", trControl=trctrl,
       tuneLength = 1, importance = TRUE, na.action=na.omit)

dengue_rf_rmse = data.frame(
dengue_rf1$results$RMSE,
dengue_rf2$results$RMSE,
dengue_rf3$results$RMSE
)
colnames(dengue_rf_rmse) = c("Random Forest 1", "Random Forest 2", "Random Forest 3")
rownames(dengue_rf_rmse) = "RMSE"
kable(dengue_rf_rmse)

dengue.forest = randomForest(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt, 
                    data=dengue_train,
                    importance = TRUE, na.action=na.omit)
```

For the random forest model, we choose the third model with the specification of `total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt` as it has the lowest in-sample cross validated rMSE. Then, I trained again the chosen model with all the training data.

```{r}
# Gradient Boosting

trctrl = trainControl(method = "cv", number = 5, savePredictions=TRUE)
dengue_gbm1 = gbm(total_cases ~., data = dengue_train, distribution = "gaussian", n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4, cv.folds = 5)
dengue_gbm2 = gbm(total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt,
      data = dengue_train, distribution = "gaussian", n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4, cv.folds = 5)
dengue_gbm3 = gbm(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt,
      data = dengue_train, distribution = "gaussian",n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4, cv.folds = 5)

dengue_gbm_rmse = data.frame(
dengue_gbm1$cv.error %>% mean %>% sqrt,
dengue_gbm2$cv.error %>% mean %>% sqrt,
dengue_gbm3$cv.error %>% mean %>% sqrt
)
colnames(dengue_gbm_rmse) = c("Gradient Boosted Tree 1", "Gradient Boosted Tree 2", "Gradient Boosted Tree 3")
rownames(dengue_gbm_rmse) = "RMSE"
kable(dengue_gbm_rmse)

dengue.gbs = gbm(total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt, 
                    data=dengue_train, distribution = "gaussian",n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4)
```

For the gradient boosted tree model, we again choose the third model with the specification of `total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt` as it has the lowest in-sample cross validated rMSE. Then, I trained again the chosen model with all the training data.

```{r}
dengue_rmse = data.frame(
modelr::rmse(dengue.tree, dengue_test),
modelr::rmse(dengue.prune, dengue_test),
modelr::rmse(dengue.forest, dengue_test),
modelr::rmse(dengue.gbs, dengue_test)
)
colnames(dengue_rmse) = c("CART", "Pruned", "Ranbdom Forest", "Boosted")
rownames(dengue_rmse) = "RMSE"
kable(dengue_rmse)
```

Out-of-sample rMSE is lowest with the random forest model. We draw the partial dependence plots below.

```{r}
# Looks like random Forest is the best and better than even the pruned tree by a slim margin
partialPlot(dengue.forest, as.data.frame(dengue_test), 'specific_humidity', las=1)
partialPlot(dengue.forest, as.data.frame(dengue_test), 'precipitation_amt', las=1)
partialPlot(dengue.forest, as.data.frame(dengue_test), 'tdtr_k', las=1)
```

## Predictive model building: green certification

```{r}
greenbuildings = read.csv("greenbuildings.csv") %>% drop_na()

greenbuildings = greenbuildings %>% mutate(green_certified = ifelse(LEED | Energystar, 1, 0) ) %>% mutate(revenue = leasing_rate*Rent)

# Splitting the data into testing and training
greenbuildings_split = initial_split(greenbuildings, 0.8)
green_train = training(greenbuildings_split)
green_test = testing(greenbuildings_split)

# Linear Regression
green_lm = lm(revenue ~ . - LEED - Energystar - leasing_rate - Rent, data=green_train)

# Variable Selection stepwise
#green_step = step(green_lm, direction = 'forward',
#                  scope = ~(. - LEED - Energystar - leasing_rate - Rent)^2)
# stepwise function chose the following model
green_step = lm(formula = revenue ~ CS_PropertyID + cluster + size + empl_gr + 
    stories + age + renovated + class_a + class_b + green_rating + 
    net + amenities + cd_total_07 + hd_total07 + total_dd_07 + 
    Precipitation + Gas_Costs + Electricity_Costs + City_Market_Rent + 
    green_certified + size:City_Market_Rent + CS_PropertyID:City_Market_Rent + 
    size:Precipitation + stories:class_a + size:Gas_Costs + cluster:City_Market_Rent + 
    green_rating:amenities + cd_total_07:hd_total07 + age:City_Market_Rent + 
    age:total_dd_07 + renovated:Precipitation + cluster:size + 
    CS_PropertyID:total_dd_07 + Electricity_Costs:City_Market_Rent + 
    renovated:Gas_Costs + CS_PropertyID:Precipitation + stories:renovated + 
    age:class_b + hd_total07:total_dd_07 + CS_PropertyID:empl_gr + 
    size:green_rating + size:class_b + size:class_a + size:age + 
    age:Electricity_Costs + renovated:City_Market_Rent + renovated:total_dd_07 + 
    class_a:City_Market_Rent + amenities:Electricity_Costs + 
    CS_PropertyID:cd_total_07 + size:renovated + empl_gr:Gas_Costs + 
    CS_PropertyID:class_b + CS_PropertyID:class_a + CS_PropertyID:size + 
    class_a:Gas_Costs + CS_PropertyID:Electricity_Costs + CS_PropertyID:cluster + 
    class_a:hd_total07 + class_a:Electricity_Costs + age:class_a + 
    class_a:Precipitation + empl_gr:renovated + cluster:Electricity_Costs + 
    cluster:hd_total07 + size:cd_total_07 + stories:cd_total_07 + 
    size:Electricity_Costs + age:Gas_Costs + class_b:Gas_Costs + 
    stories:age + renovated:Electricity_Costs + cd_total_07:total_dd_07 + 
    age:cd_total_07 + hd_total07:Electricity_Costs + stories:Precipitation + 
    amenities:Gas_Costs + amenities:Precipitation, data = green_train)

# Trees
green.tree = rpart(revenue ~ . - LEED - Energystar - leasing_rate - Rent, 
                    data=green_train,
                    control = rpart.control(cp = 0.002, minsplit=30))

# Tree with pruning
# cp_1se(green.tree)
green.prune = prune_1se(green.tree)

# Random Forest
green.forest = randomForest(revenue ~ . - LEED - Energystar - leasing_rate - Rent, 
                    data=green_train,
                    importance = TRUE, na.action=na.omit)

# Gradient Boosting
green.gbs = gbm(revenue ~ . - LEED - Energystar - leasing_rate - Rent, 
                    data=green_train, distribution = "gaussian", n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4)

# knn model
k_grid = seq(300,495, by=5)

green_knn_grid = foreach(k = k_grid, .combine='rbind') %do% {
  model = knnreg(revenue ~ . - LEED - Energystar - leasing_rate - Rent, k=k, data = green_train)
  rms = modelr::rmse(model, green_test)
  c(k=k, err=rms)
} %>% as.data.frame

green_knn_final = green_knn_grid %>% filter(err == min(green_knn_grid$err))

green_rmse = data.frame(
modelr::rmse(green_lm, green_test),
modelr::rmse(green_step, green_test),
modelr::rmse(green.tree, green_test),
modelr::rmse(green.prune, green_test),
modelr::rmse(green.forest, green_test),
modelr::rmse(green.gbs, green_test),
green_knn_final$err)
colnames(green_rmse) = c("Linear", "Stepwise", "Tree", "Pruned Tree", "Forest", "GBM", "knn")
rownames(green_rmse) = "RMSE"
kable(green_rmse)

#partialPlot(green.forest, as.data.frame(green_test), 'green_certified', las=1)
```

Modeling started with combining the LEED and EnergyStar to create a green_certified 
column. Also removed the null variables. Created a train test split with 80 percent
of the data being the training set data and 20 percent being the testing set data.

In terms of the types of models we started with a linear regression model. After that
using the stepwise variable selection computed the best set of variables and the
interaction between them which performed the best. That became the second model. Next,
experimented with a tree model by considering all the variables except LEED, Energystar
because LEED and Energystar is already considered under the green_certified and including
it would create multicollinearity issues. We also adopted the pruning method to test
trees with different configurations which increased the performance of the tree model.
Used the random forest models without the variables LEED and Energystar as the next model
and saw a significant improvment in the model performance. Gradient Boosting model was
selected next with distribution as "gaussian", the number of trees as 10000, shrinkage as 0.01,
and with a interaction depth of 4. This model gave a similar performance as the random forest 
model. At last we chose a knn model and calculated a model for each k value from 300 to 495 with
a difference of 5. Found the best k value which has the best performance.

In the end compared all these selected models with the rmse values and this can be seen on the
table above. 

Since the random forest model performed the best, chose this model to draw a partial plot with.
As seen in the graph above a green_certified building generates 3 dollars more in revenue on
average compared to a non green_certified building per square foot per year. 

## Predictive model building: California housing

```{r}
CAhousing = read_csv("CAhousing.csv")
CAmap = get_map("california", zoom=4)

CAhousing_split = initial_split(CAhousing, 0.8)
CAhousing_train = training(CAhousing_split)
CAhousing_test = testing(CAhousing_split)

# Linear Regression
CAhousing_lm = lm(medianHouseValue ~ . - longitude - latitude, data=CAhousing_train)

# Stepwise
CAhousing_step = step(CAhousing_lm, scope=~(. - longitude - latitude)^2)

# CART
CAhousing.tree = rpart(medianHouseValue ~ housingMedianAge + totalRooms + 
                      totalBedrooms + population + households + medianIncome, 
                    data=CAhousing_train,
                    control = rpart.control(cp = 0.002, minsplit=30))

#rpart.plot(CAhousing.tree, digits=-5, type=4, extra=1)

cp_1se(CAhousing.tree)

CAhousing.prune = prune_1se(CAhousing.tree)

# Random Forest
CAhousing.forest = randomForest(medianHouseValue ~ housingMedianAge + totalRooms + 
                      totalBedrooms + population + households + medianIncome, 
                    data=CAhousing_train,
                    importance = TRUE, na.action=na.omit)

#plotcp(CAhousing.tree)

# Gradient Boosting
CAhousing.gbs = gbm(medianHouseValue ~ housingMedianAge + totalRooms + 
                      totalBedrooms + population + households + medianIncome, 
                    data=CAhousing_train, distribution = "gaussian", n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4)

CAhousing_rmse = data.frame(
modelr::rmse(CAhousing_lm, CAhousing_test),
modelr::rmse(CAhousing_step, CAhousing_test),
modelr::rmse(CAhousing.tree, CAhousing_test),
modelr::rmse(CAhousing.prune, CAhousing_test),
modelr::rmse(CAhousing.forest, CAhousing_test),
modelr::rmse(CAhousing.gbs, CAhousing_test))
colnames(CAhousing_rmse) = c("Linear", "Stepwise", "Tree", "Pruned Tree", "Forest", "GBM")
rownames(CAhousing_rmse) = "RMSE"
kable(CAhousing_rmse)
#proceed with GBM model
```

We choose the gradiented boosted model that has `housingMedianAge`, `totalRooms`, `totalBedrooms`, `population`, `households`, and `medianIncome` as the features. The plots are below.

```{r}
CAmap = get_map("california", zoom=6)
```

```{r}
ggmap(CAmap) + 
  geom_point(aes(x = longitude, y = latitude, color = medianHouseValue/100000), data = CAhousing, alpha = .4) +
  labs(x = 'Longitude', y = 'Latitude', title = "Median House Values in California", 
       color = 'in 100,000 Dollars')
```

```{r}
CApredict = CAhousing %>% mutate(predictedMedianHouseValue = predict.gbm(CAhousing.gbs, newdata = CAhousing))
ggmap(CAmap) + 
  geom_point(aes(x = longitude, y = latitude, color = predictedMedianHouseValue/100000), data = CApredict, alpha = .4) +
  labs(x = 'Longitude', y = 'Latitude', title = "Predicted Median House Values in California", 
       color = 'in 100,000 Dollars')
```

```{r}
CApredict = CApredict %>% mutate(residuals = medianHouseValue - predictedMedianHouseValue)
ggmap(CAmap) + 
  geom_point(aes(x = longitude, y = latitude, color = residuals/100000), data = CApredict, alpha = .4) +
  labs(x = 'Longitude', y = 'Latitude', title = "Residuals of the Predictions",
       color = 'in 100,000 Dollars')
```

