---
title: "ECO395M: Exercise 3"
author: "Steven Kim and Shreekara Shastry"
date: ""
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(knitr)
library(rsample)
library(pdp)
library(rpart)
library(rpart.plot)
library(gbm)
library(randomForest)
library(foreach)
library(ggmap)
library(caret)
```

## What causes what?

1. You can't just get data from a few different cities and run the regressions of “Crime” on “Police” because cities have incentives to hire more cops when there is an increased number of crimes. Because of this, it would look like “Crime” is positively correlated to “Police” when there is no reason to believe there is a causal relationship.

How were the researchers from UPenn able to isolate this effect? Briefly describe their approach and discuss their result in the “Table 2” below, from the researchers' paper.

2. The researchers from UPenn were able to isolate this effect because the District of Columbia had the policy where they increased  “Police” when there is an increased risk of terrorism, which is believed to be unrelated to the street crime rates. The result in the “Table 2” below says that there is a statistically significant negative relationship between the “High Alert” and “Crime”, implying that the increased number of cops because of the possible terrorist risk decreased the crime rates. This holds true even after controlling for the ridership of Metro.

3. They had to control for Metro ridership because, if “Crime” decreased because there were less people on the streets, that would not neccesarily mean the rate of crime decreasing because of the increased number of cops. This would be of concern if people stayed home because of the terrorist alert. However, it did not turn out to be true. They were trying to capture the effect of the decrease of normal human activity in the city on the number of crime incidents.

4. The model being estimated here is the effect of “High Alert”, controlled for “Midday Ridership”, by districts (if it is district 1 or not). The conclusion is that the effect of “High Alert” is only significant in the first police district area.


## Tree modeling: dengue cases

```{r}
dengue = read_csv('dengue.csv') %>% drop_na()

dengue$city = dengue$city %>% factor()
dengue$season = dengue$season %>% factor()

dengue_split = initial_split(dengue, 0.8)
dengue_train = training(dengue_split)
dengue_test = testing(dengue_split)

#including city didn't really help

# CART
dengue.tree = rpart(total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt, 
                    data=dengue_train,
                    control = rpart.control(cp = 0.002, minsplit=30))

# rpart.plot(dengue.tree, digits=-5, type=4, extra=1)

cp_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  cp_opt
}

# cp_1se(dengue.tree)

prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}

dengue.prune = prune_1se(dengue.tree)

# Random Forest
dengue.forest = randomForest(total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt, 
                    data=dengue_train,
                    importance = TRUE, na.action=na.omit)

# plotcp(dengue.tree)

# Gradient Boosting
dengue.gbs = gbm(total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt, 
                    data=dengue_train, distribution = "gaussian",n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4)

modelr::rmse(dengue.tree, dengue_test)
modelr::rmse(dengue.prune, dengue_test)
modelr::rmse(dengue.forest, dengue_test)
modelr::rmse(dengue.gbs, dengue_test)

# Looks like random Forest is the best and better than even the pruned tree by a slim margin
partialPlot(dengue.forest, as.data.frame(dengue_test), 'specific_humidity', las=1)
partialPlot(dengue.forest, as.data.frame(dengue_test), 'precipitation_amt', las=1)
partialPlot(dengue.forest, as.data.frame(dengue_test), 'tdtr_k', las=1)
```

## Predictive model building: green certification

```{r}
greenbuildings = read.csv("greenbuildings.csv") %>% drop_na()

greenbuildings = greenbuildings %>% mutate(green_certified = ifelse(LEED | Energystar, 1, 0) ) %>% mutate(revenue = leasing_rate*Rent)

# Splitting the data into testing and training
greenbuildings_split = initial_split(greenbuildings, 0.8)
green_train = training(greenbuildings_split)
green_test = testing(greenbuildings_split)

# Linear Regression
green_lm = lm(revenue ~ . - LEED - Energystar - leasing_rate - Rent, data=green_train)

# Variable Selection stepwise
green_step = step(green_lm, direction = 'forward',
                  scope = ~(. - LEED - Energystar - leasing_rate - Rent)^2)

# Trees
green.tree = rpart(revenue ~ . - LEED - Energystar - leasing_rate - Rent, 
                    data=green_train,
                    control = rpart.control(cp = 0.002, minsplit=30))

# Tree with pruning
# cp_1se(green.tree)
green.prune = prune_1se(green.tree)

# Random Forest
green.forest = randomForest(revenue ~ . - LEED - Energystar - leasing_rate - Rent, 
                    data=green_train,
                    importance = TRUE, na.action=na.omit)

# Gradient Boosting
green.gbs = gbm(revenue ~ . - LEED - Energystar - leasing_rate - Rent, 
                    data=green_train, distribution = "gaussian", n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4)

# knn model
k_grid = seq(300,495, by=5)

green_knn_grid = foreach(k = k_grid, .combine='rbind') %do% {
  model = knnreg(revenue ~ . - LEED - Energystar - leasing_rate - Rent, k=k, data = green_train)
  rms = modelr::rmse(model, green_test)
  c(k=k, err=rms)
} %>% as.data.frame

green_knn_final = green_knn_grid %>% filter(err == min(green_knn_grid$err))

green_rmse = data.frame(
modelr::rmse(green_lm, green_test),
modelr::rmse(green_step, green_test),
modelr::rmse(green.tree, green_test),
modelr::rmse(green.prune, green_test),
modelr::rmse(green.forest, green_test),
modelr::rmse(green.gbs, green_test),
green_knn_final$err)
colnames(green_rmse) = c("Linear", "Stepwise", "Tree", "Pruned Tree", "Forest", "GBS", "knn")
rownames(green_rmse) = "RMSE"
kable(green_rmse)

#partialPlot(green.forest, as.data.frame(green_test), 'green_certified', las=1)
```


## Predictive model building: California housing

```{r}
library(ggmap)
CAhousing = read_csv("CAhousing.csv")
CAmap = get_map("california", zoom=4)

CAhousing_split = initial_split(CAhousing, 0.8)
CAhousing_train = training(CAhousing_split)
CAhousing_test = testing(CAhousing_split)

# Linear Regression
CAhousing_lm = lm(medianHouseValue ~ . - longitude - latitude, data=CAhousing_train)

# Stepwise
CAhousing_step = step(CAhousing_lm, scope=~(. - longitude - latitude)^2)

# CART
CAhousing.tree = rpart(medianHouseValue ~ housingMedianAge + totalRooms + 
                      totalBedrooms + population + households + medianIncome, 
                    data=CAhousing_train,
                    control = rpart.control(cp = 0.002, minsplit=30))

#rpart.plot(CAhousing.tree, digits=-5, type=4, extra=1)

cp_1se(CAhousing.tree)

CAhousing.prune = prune_1se(CAhousing.tree)

# Random Forest
CAhousing.forest = randomForest(medianHouseValue ~ housingMedianAge + totalRooms + 
                      totalBedrooms + population + households + medianIncome, 
                    data=CAhousing_train,
                    importance = TRUE, na.action=na.omit)

#plotcp(CAhousing.tree)

# Gradient Boosting
CAhousing.gbs = gbm(medianHouseValue ~ housingMedianAge + totalRooms + 
                      totalBedrooms + population + households + medianIncome, 
                    data=CAhousing_train, distribution = "gaussian", n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4)

CAhousing_rmse = data.frame(
modelr::rmse(CAhousing_lm, CAhousing_test),
modelr::rmse(CAhousing_step, CAhousing_test),
modelr::rmse(CAhousing.tree, CAhousing_test),
modelr::rmse(CAhousing.prune, CAhousing_test),
modelr::rmse(CAhousing.forest, CAhousing_test),
modelr::rmse(CAhousing.gbs, CAhousing_test))
colnames(CAhousing_rmse) = c("Linear", "Stepwise", "Tree", "Pruned Tree", "Forest", "GBS")
rownames(CAhousing_rmse) = "RMSE"
kable(CAhousing_rmse)
#proceed with GBS model
```

```{r}
CAmap = get_map("california", zoom=6)
ggmap(CAmap) + 
  geom_point(aes(x = longitude, y = latitude, color = medianHouseValue), data = CAhousing, alpha = .4) +
  labs(x = 'Longitude', y = 'Latitude', title = "",
       subtitle = '')
```

```{r}
CApredict = CAhousing %>% mutate(predictedMedianHouseValue = predict.gbm(CAhousing.gbs, newdata = CAhousing))
ggmap(CAmap) + 
  geom_point(aes(x = longitude, y = latitude, color = predictedMedianHouseValue), data = CApredict, alpha = .4) +
  labs(x = 'Longitude', y = 'Latitude', title = "",
       subtitle = '')
```

```{r}
CApredict = CApredict %>% mutate(residuals = medianHouseValue - predictedMedianHouseValue)
ggmap(CAmap) + 
  geom_point(aes(x = longitude, y = latitude, color = residuals), data = CApredict, alpha = .4) +
  labs(x = 'Longitude', y = 'Latitude', title = "",
       subtitle = '')
```

