---
title: "ECO395M: Exercise 3"
author: "Steven Kim and Shreekara Shastry"
date: ""
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(knitr)
library(rsample)
library(pdp)
library(rpart)
library(rpart.plot)
library(gbm)
library(randomForest)
library(foreach)
library(ggmap)
library(caret)
```

## What causes what?

1. You can't just get data from a few different cities and run the regressions of “Crime” on “Police” because cities have incentives to hire more cops when there is an increased number of crimes. Because of this, it would look like “Crime” is positively correlated to “Police” when there is no reason to believe there is a causal relationship.

2. The researchers from UPenn were able to isolate this effect because the District of Columbia had the policy where they increased  “Police” when there is an increased risk of terrorism, which is believed to be unrelated to the street crime rates. The result in the “Table 2” below says that there is a statistically significant negative relationship between the “High Alert” and “Crime”, implying that the increased number of cops because of the possible terrorist risk decreased the crime rates. This holds true even after controlling for the ridership of Metro.

3. They had to control for Metro ridership because, if “Crime” decreased because there were less people on the streets, that would not neccesarily mean the rate of crime decreasing because of the increased number of cops. This would be of concern if people stayed home because of the terrorist alert. However, it did not turn out to be true. They were trying to capture the effect of the decrease of normal human activity in the city on the number of crime incidents.

4. The model being estimated here is the effect of “High Alert”, controlled for “Midday Ridership”, by districts (if it is district 1 or not). The conclusion is that the effect of “High Alert” is only significant in the first police district area.


## Tree modeling: dengue cases

```{r}
dengue = read_csv('dengue.csv') %>% drop_na()

dengue$city = dengue$city %>% factor()
dengue$season = dengue$season %>% factor()

set.seed(98236)
dengue_split = initial_split(dengue, 0.8)
dengue_train = training(dengue_split)
dengue_test = testing(dengue_split)

#CART
trctrl = trainControl(method = "cv", number = 5, savePredictions=TRUE)
dengue_tree1 = train(total_cases ~., data = dengue_train, method = "rpart", trControl=trctrl, 
                     tuneLength = 1, control = rpart.control(cp = 0.002, minsplit=30))
dengue_tree2 = train(total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt,
      data = dengue_train, method = "rpart", trControl=trctrl, tuneLength = 1, 
      control = rpart.control(cp = 0.002, minsplit=30))
dengue_tree3 = train(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt,
      data = dengue_train, method = "rpart", trControl=trctrl, tuneLength = 1,
      control = rpart.control(cp = 0.002, minsplit=30))

dengue_tree_rmse = data.frame(
dengue_tree1$results$RMSE,
dengue_tree2$results$RMSE,
dengue_tree3$results$RMSE
)
colnames(dengue_tree_rmse) = c("Tree 1", "Tree 2", "Tree 3")
rownames(dengue_tree_rmse) = "RMSE"
kable(dengue_tree_rmse)

# CART, with all the training data
dengue.tree = rpart(total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt, 
                    data=dengue_train,
                    control = rpart.control(cp = 0.002, minsplit=30))

prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}

dengue.prune = prune_1se(dengue.tree)
```

First, I split the data into training and testing sets. Then, I wanted to choose the best CART model. For CART, we choose the second model with the specification of `total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt` as it has the lowest in-sample cross validated rMSE. Then, I trained again the chosen model with all the training data and pruned the tree.

```{r}
# Random Forest

trctrl = trainControl(method = "cv", number = 5, savePredictions=TRUE)
dengue_rf1 = train(total_cases ~., data = dengue_train, method = "rf", trControl=trctrl,
                   tuneLength = 1, importance = TRUE, na.action=na.omit)
dengue_rf2 = train(total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt,
      data = dengue_train, method = "rf", trControl=trctrl,
       tuneLength = 1, importance = TRUE, na.action=na.omit)
dengue_rf3 = train(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt,
      data = dengue_train, method = "rf", trControl=trctrl,
       tuneLength = 1, importance = TRUE, na.action=na.omit)

dengue_rf_rmse = data.frame(
dengue_rf1$results$RMSE,
dengue_rf2$results$RMSE,
dengue_rf3$results$RMSE
)
colnames(dengue_rf_rmse) = c("Random Forest 1", "Random Forest 2", "Random Forest 3")
rownames(dengue_rf_rmse) = "RMSE"
kable(dengue_rf_rmse)

dengue.forest = randomForest(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt, 
                    data=dengue_train,
                    importance = TRUE, na.action=na.omit)
```

For the random forest model, we choose the third model with the specification of `total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt` as it has the lowest in-sample cross validated rMSE. Then, I trained again the chosen model with all the training data.

```{r}
# Gradient Boosting

dengue_gbm1 = gbm(total_cases ~., data = dengue_train, distribution = "gaussian", n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4, cv.folds = 5)
dengue_gbm2 = gbm(total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt,
      data = dengue_train, distribution = "gaussian", n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4, cv.folds = 5)
dengue_gbm3 = gbm(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt,
      data = dengue_train, distribution = "gaussian",n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4, cv.folds = 5)

dengue_gbm_rmse = data.frame(
dengue_gbm1$cv.error %>% mean %>% sqrt,
dengue_gbm2$cv.error %>% mean %>% sqrt,
dengue_gbm3$cv.error %>% mean %>% sqrt
)
colnames(dengue_gbm_rmse) = c("Gradient Boosted Tree 1", "Gradient Boosted Tree 2", "Gradient Boosted Tree 3")
rownames(dengue_gbm_rmse) = "RMSE"
kable(dengue_gbm_rmse)

dengue.gbs = gbm(total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt, 
                    data=dengue_train, distribution = "gaussian",n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4)
```

For the gradient boosted tree model, we again choose the third model with the specification of `total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt` as it has the lowest in-sample cross validated rMSE. Then, I trained again the chosen model with all the training data.

```{r}
dengue_rmse = data.frame(
modelr::rmse(dengue.tree, dengue_test),
modelr::rmse(dengue.prune, dengue_test),
modelr::rmse(dengue.forest, dengue_test),
modelr::rmse(dengue.gbs, dengue_test)
)
colnames(dengue_rmse) = c("CART", "Pruned", "Ranbdom Forest", "Boosted")
rownames(dengue_rmse) = "RMSE"
kable(dengue_rmse)
```

Out-of-sample rMSE is lowest with the random forest model. We draw the partial dependence plots on `specific_humidity`, `precipitation_amt`, and `tdtr_k` below.

```{r}
# Looks like random Forest is the best and better than even the pruned tree by a slim margin
partialPlot(dengue.forest, as.data.frame(dengue_test), 'specific_humidity', las=1)
partialPlot(dengue.forest, as.data.frame(dengue_test), 'precipitation_amt', las=1)
partialPlot(dengue.forest, as.data.frame(dengue_test), 'tdtr_k', las=1)
```

## Predictive model building: green certification

```{r}
greenbuildings = read.csv("greenbuildings.csv") %>% drop_na()

greenbuildings = greenbuildings %>% mutate(green_certified = ifelse(LEED | Energystar, 1, 0) ) %>% mutate(revenue = leasing_rate*Rent)

# Splitting the data into testing and training
greenbuildings_split = initial_split(greenbuildings, 0.8)
green_train = training(greenbuildings_split)
green_test = testing(greenbuildings_split)
```

We started modeling with combining the LEED and EnergyStar to create a green_certified 
column, which is a dummy variable that is 1 if green certified in any form and 0 otherwise.
Also, we removed the nulls. Then, we created a train/test split with 80 percent
of the data being the training set data and 20 percent being the testing set data.
In the analysis, we do not use `CS_PropertyID` variable as it is just the unique id's for each property.

```{r}
# Linear Regression
green_lm = lm(revenue ~ . - LEED - Energystar - leasing_rate - Rent - CS_PropertyID, data=green_train)

green_lm_wo = lm(revenue ~ . - LEED - Energystar - CS_PropertyID, data=green_train)

# Variable Selection stepwise
#green_step = step(green_lm, direction = 'forward',
#                  scope = ~(. - LEED - Energystar - leasing_rate - Rent - CS_PropertyID)^2)
# stepwise function chose the following model
green_step = lm(formula = revenue ~ cluster + size + empl_gr + 
    stories + age + renovated + class_a + class_b + green_rating + 
    net + amenities + cd_total_07 + hd_total07 + total_dd_07 + 
    Precipitation + Gas_Costs + Electricity_Costs + City_Market_Rent + 
    green_certified + size:City_Market_Rent + 
    size:Precipitation + stories:class_a + size:Gas_Costs + cluster:City_Market_Rent + 
    green_rating:amenities + cd_total_07:hd_total07 + age:City_Market_Rent + 
    age:total_dd_07 + renovated:Precipitation + cluster:size + 
    Electricity_Costs:City_Market_Rent + 
    renovated:Gas_Costs + stories:renovated + 
    age:class_b + hd_total07:total_dd_07 + 
    size:green_rating + size:class_b + size:class_a + size:age + 
    age:Electricity_Costs + renovated:City_Market_Rent + renovated:total_dd_07 + 
    class_a:City_Market_Rent + amenities:Electricity_Costs + 
    size:renovated + empl_gr:Gas_Costs + 
    class_a:Gas_Costs + 
    class_a:hd_total07 + class_a:Electricity_Costs + age:class_a + 
    class_a:Precipitation + empl_gr:renovated + cluster:Electricity_Costs + 
    cluster:hd_total07 + size:cd_total_07 + stories:cd_total_07 + 
    size:Electricity_Costs + age:Gas_Costs + class_b:Gas_Costs + 
    stories:age + renovated:Electricity_Costs + cd_total_07:total_dd_07 + 
    age:cd_total_07 + hd_total07:Electricity_Costs + stories:Precipitation + 
    amenities:Gas_Costs + amenities:Precipitation, data = green_train)
```

In terms of the types of models, we started with a baseline linear regression model, with the specification of 
revenue on everything else. 
After that, using the stepwise variable selection function, we computed the best set of variables and the
interaction between them which performed the best. The linear model chose by the stepwise function is 
`revenue ~ cluster + size + empl_gr + 
    stories + age + renovated + class_a + class_b + green_rating + 
    net + amenities + cd_total_07 + hd_total07 + total_dd_07 + 
    Precipitation + Gas_Costs + Electricity_Costs + City_Market_Rent + 
    green_certified + size:City_Market_Rent + 
    size:Precipitation + stories:class_a + size:Gas_Costs + cluster:City_Market_Rent + 
    green_rating:amenities + cd_total_07:hd_total07 + age:City_Market_Rent + 
    age:total_dd_07 + renovated:Precipitation + cluster:size + 
    Electricity_Costs:City_Market_Rent + 
    renovated:Gas_Costs + :Precipitation + stories:renovated + 
    age:class_b + hd_total07:total_dd_07 + 
    size:green_rating + size:class_b + size:class_a + size:age + 
    age:Electricity_Costs + renovated:City_Market_Rent + renovated:total_dd_07 + 
    class_a:City_Market_Rent + amenities:Electricity_Costs + 
    size:renovated + empl_gr:Gas_Costs + 
    class_a:Gas_Costs + 
    class_a:hd_total07 + class_a:Electricity_Costs + age:class_a + 
    class_a:Precipitation + empl_gr:renovated + cluster:Electricity_Costs + 
    cluster:hd_total07 + size:cd_total_07 + stories:cd_total_07 + 
    size:Electricity_Costs + age:Gas_Costs + class_b:Gas_Costs + 
    stories:age + renovated:Electricity_Costs + cd_total_07:total_dd_07 + 
    age:cd_total_07 + hd_total07:Electricity_Costs + stories:Precipitation + 
    amenities:Gas_Costs + amenities:Precipitation`. We decided to take both models to the final decision where we compare the 
out-of-sample RMSE's.


Next, we experimented with a tree model by considering all the variables except LEED, Energystar
because LEED and Energystar is already considered under the green_certified (tree 3).
We then constructed 2 more tree models, one having all the variables and the other 
one without leasing_rate, Rent, LEED and Energystar as features (tree 1 and 2).
The specifications are `revenue ~ . - CS_PropertyID`, `revenue ~ . - LEED - Energystar - leasing_rate - Rent - CS_PropertyID`, and
`revenue ~ . - LEED - Energystar - CS_PropertyID` for each model 1, 2, and 3.
We compared the three models with cross validated in-sample rMSE's with the fold of 5.
The table for the rMSE's are below. As the one that takes all the variables into account has the 
lowest rMSE, we choose this model to take to the final decision.
We then also pruned the tree to see if this would increase the performance.

```{r}
# Trees
trctrl = trainControl(method = "cv", number = 5, savePredictions=TRUE)
green_tree1 = train(revenue ~ . - CS_PropertyID, data = green_train, method = "rpart", trControl=trctrl, tuneLength = 0)
green_tree2 = train(revenue ~ . - LEED - Energystar - leasing_rate - Rent - CS_PropertyID,
      data = green_train, method = "rpart", trControl=trctrl, tuneLength = 0)
green_tree3 = train(revenue ~ . - LEED - Energystar - CS_PropertyID,
      data = green_train, method = "rpart", trControl=trctrl, tuneLength = 0)

green_tree_rmse = data.frame(
green_tree1$results$RMSE,
green_tree2$results$RMSE,
green_tree3$results$RMSE
)
colnames(green_tree_rmse) = c("Tree 1", "Tree 2", "Tree 3")
rownames(green_tree_rmse) = "RMSE"
kable(green_tree_rmse)

green.tree = rpart(revenue ~ . - LEED - Energystar - CS_PropertyID, 
                    data=green_train,
                    control = rpart.control(cp = 0.002, minsplit=30))

green.prune = prune_1se(green.tree)
```

For random forest models, we also started with three models that utilize different features as in choosing the tree model.
The specifications are `revenue ~ . - CS_PropertyID`, `revenue ~ . - LEED - Energystar - leasing_rate - Rent - CS_PropertyID`, and
`revenue ~ . - LEED - Energystar - CS_PropertyID` for each model 1, 2, and 3.
Out of the three models, the model which had all the variables performed
the best during cross-validated in-sample performance test.
The table for the rMSE's for each forest model is below.

```{r}
# Random Forest
trctrl = trainControl(method = "cv", number = 5, savePredictions=TRUE)
green_forest1 = train(revenue ~. - CS_PropertyID, data = green_train, method = "rf", trControl=trctrl, prox=TRUE, tuneLength=1)
green_forest2 = train(revenue ~ . - LEED - Energystar - leasing_rate - Rent - CS_PropertyID,
      data = green_train, method = "rf", trControl=trctrl, prox=TRUE, tuneLength=1)
green_forest3 = train(revenue ~ . - LEED - Energystar - CS_PropertyID,
      data = green_train, method = "rf", trControl=trctrl, prox=TRUE, tuneLength=1)

green_forest_rmse = data.frame(
green_forest1$results$RMSE,
green_forest2$results$RMSE,
green_forest3$results$RMSE
)
colnames(green_forest_rmse) = c("Random Forest 1", "Random Forest 2", "Random Forest 3")
rownames(green_forest_rmse) = "RMSE"
kable(green_forest_rmse)

green.forest = randomForest(revenue ~ . - LEED - Energystar - CS_PropertyID, 
                    data=green_train,
                    importance = TRUE, na.action=na.omit)
```

We repeated the process for gradient boosted models.
We used Gradient Boosting models with distribution as "gaussian", the number of trees 
as 10000, shrinkage as 0.01,and with a interaction depth of 4.
Out of the three models, the model which had all the variables performed
the best during cross-validated in-sample performance test.
The table for the rMSE's for each boosted model is below.

```{r}
# Gradient Boosting
green_gbm1 = gbm(revenue ~. - CS_PropertyID, data = green_train, distribution = "gaussian", n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4, cv.folds = 5)
green_gbm2 = gbm(revenue ~ . - LEED - Energystar - leasing_rate - Rent - CS_PropertyID,
      data = green_train, distribution = "gaussian", n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4, cv.folds = 5)
green_gbm3 = gbm(revenue ~ . - LEED - Energystar - CS_PropertyID, 
                    data=green_train, distribution = "gaussian", n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4, cv.folds = 5)

green_gbm_rmse = data.frame(
green_gbm1$cv.error %>% mean %>% sqrt,
green_gbm2$cv.error %>% mean %>% sqrt,
green_gbm3$cv.error %>% mean %>% sqrt
)
colnames(green_gbm_rmse) = c("Boosted 1", "Boosted 2", "Boosted 3")
rownames(green_gbm_rmse) = "RMSE"
kable(green_gbm_rmse)

green.gbs = gbm(revenue ~ . - LEED - Energystar - CS_PropertyID, 
                    data=green_train, distribution = "gaussian", n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4, cv.folds = 5)
```

We repeat the process for knn models.
Out of the same three models, the model which had all the variables except for `LEED, Energystar, leasing_rate,` and `Rent` with 
`k = 5` performed the best during cross-validated in-sample performance test.
The table for the rMSE's for each boosted model is below.

```{r}
# knn model
trctrl = trainControl(method = "cv", number = 5, savePredictions=TRUE)
green_knn1 = train(revenue ~.- CS_PropertyID, data = green_train, method = "knn", trControl=trctrl, tuneLength=20)
green_knn2 = train(revenue ~ . - LEED - Energystar - leasing_rate - Rent- CS_PropertyID,
                   data = green_train, method = "knn", trControl=trctrl, tuneLength=20)
green_knn3 = train(revenue ~ . - LEED - Energystar- CS_PropertyID,
                   data = green_train, method = "knn", trControl=trctrl, tuneLength=20)

green_knn_rmse = data.frame(matrix(c(
green_knn1$results[green_knn1$results$RMSE == green_knn1$results$RMSE %>% min, 1] %>% as.integer(), 
green_knn1$results[green_knn1$results$RMSE == green_knn1$results$RMSE %>% min, 2],
green_knn2$results[green_knn2$results$RMSE == green_knn2$results$RMSE %>% min, 1] %>% as.integer(), 
green_knn2$results[green_knn2$results$RMSE == green_knn2$results$RMSE %>% min, 2],
green_knn3$results[green_knn3$results$RMSE == green_knn3$results$RMSE %>% min, 1] %>% as.integer(), 
green_knn3$results[green_knn3$results$RMSE == green_knn3$results$RMSE %>% min, 2]),
nrow = 2))
colnames(green_knn_rmse) = c("knn 1", "knn 2", "knn 3")
rownames(green_knn_rmse) = c('k', "RMSE")
kable(green_knn_rmse)

green.knn = knnreg(revenue ~ . - LEED - Energystar - leasing_rate - Rent- CS_PropertyID, k=5, data = green_train)
```

Finally, we chose tree 3 and knn 3 as it has the lowest in-sample cross-validated RMSE's.
For the random forest and gradient boosted models, we choose the 3's although 1' perform slightly better.
This is because we're trying to answer the question of the effect of getting the green certification 
on the revenue per square foot per calendar year and including the two variables in the model woud dilute the partial dependence.

In the end, we trained the chosen models with all the training data and 
the out-of sample rmse values. The table for this is below.

```{r}
green_rmse = data.frame(
modelr::rmse(green_lm, green_test),
modelr::rmse(green_step, green_test),
modelr::rmse(green_tree1, green_test),
modelr::rmse(green.prune, green_test),
modelr::rmse(green_forest1, green_test),
modelr::rmse(green.gbs, green_test),
modelr::rmse(green.knn, green_test))
colnames(green_rmse) = c("Linear", "Stepwise", "Tree", "Pruned Tree", "Forest", "GBM", "knn")
rownames(green_rmse) = "RMSE"
kable(green_rmse)
```

Since the gradient boosted model performed the best, we calculated the difference of the predicted revenue for `green_certified == 1` and `green_certified == 0` to calculate the partial dependence of the green certification. 
On average, a green_certified building generates `r filter(green_test, green_certified == 1)[, 'revenue'] %>% mean - 
filter(green_test, green_certified == 0)[, 'revenue'] %>% mean` dollars more in revenue, per square foot per year, 
compared to a non green_certified building.

```{r}
#partialPlot(green.forest, as.data.frame(green_test), 'green_certified', las=1)
green_test = green_test %>% mutate(predicted = predict(green.gbs, newdata = green_test, type = 'response'))
filter(green_test, green_certified == 1)[, 'revenue'] %>% mean - 
filter(green_test, green_certified == 0)[, 'revenue'] %>% mean
```

## Predictive model building: California housing

We start by splitting the dataset into training and testing set. 
Training set takes 80% of the dataset and rest going to the testing set. 
In terms of the model selection, we started with a baseline linear model. 
To select the statistically significant variables, we created a stepwise selection model with all the variables except latitude and longitude in the scope. 
After this, we built tree, random forest and gradient boosted models with `housingMedianAge, totalRooms, totalBedrooms, population, households, medianIncome` variables and all the variables including `longitude` and `latitude`. 
We also pruned the tree models to check if it produces a better model. 
It turned out that pruned tree was not better than the tree model. 
The table of in-sample cross-validated RMSE's for each model is below.

```{r}
CAhousing = read_csv("CAhousing.csv")

CAhousing_split = initial_split(CAhousing, 0.8)
CAhousing_train = training(CAhousing_split)
CAhousing_test = testing(CAhousing_split)

# Linear Regression
CAhousing_lm = lm(medianHouseValue ~ . - longitude - latitude, data=CAhousing_train)

# Stepwise
CAhousing_step = step(CAhousing_lm, scope=~(. - longitude - latitude)^2, trace=0)

# CART
trctrl = trainControl(method = "cv", number = 5, savePredictions=TRUE)
CAhouse_tree1 = train(medianHouseValue ~., data = CAhousing_train, method = "rpart", trControl=trctrl, tuneLength = 1)
CAhouse_tree2 = train(medianHouseValue ~ housingMedianAge + totalRooms + 
                      totalBedrooms + population + households + medianIncome, 
                    data=CAhousing_train, method = "rpart", trControl=trctrl, tuneLength = 1)

CAhouse_tree_rmse = data.frame(
CAhouse_tree1$results$RMSE,
CAhouse_tree2$results$RMSE
)
colnames(CAhouse_tree_rmse) = c("Tree 1", "Tree 2")
rownames(CAhouse_tree_rmse) = "RMSE"
kable(CAhouse_tree_rmse)

CAhousing.tree = rpart(medianHouseValue ~ ., 
                    data=CAhousing_train,
                    control = rpart.control(cp = 0.002, minsplit=30))

CAhousing.prune = prune_1se(CAhousing.tree)

```

```{r}
# Random Forest
trctrl = trainControl(method = "cv", number = 5, savePredictions=TRUE)
CAhouse_rf1 = train(medianHouseValue ~., data = CAhousing_train, method = "rf", trControl=trctrl, tuneLength = 1)
CAhouse_rf2 = train(medianHouseValue ~ housingMedianAge + totalRooms + 
                      totalBedrooms + population + households + medianIncome, 
                    data=CAhousing_train, method = "rf", trControl=trctrl, tuneLength = 1)

CAhouse_rf_rmse = data.frame(
CAhouse_rf1$results$RMSE,
CAhouse_rf2$results$RMSE
)
colnames(CAhouse_rf_rmse) = c("Random Forest 1", "Random Forest 2")
rownames(CAhouse_rf_rmse) = "RMSE"
kable(CAhouse_rf_rmse)

CAhousing.forest = randomForest(medianHouseValue ~ .,
                    data=CAhousing_train,
                    importance = TRUE, na.action=na.omit)
```

```{r}
# Gradient Boosting
CAhousing_gbm1 = gbm(medianHouseValue ~ ., 
                    data=CAhousing_train, distribution = "gaussian", n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4, cv.folds = 5)
CAhousing_gbm2 = gbm(medianHouseValue ~ housingMedianAge + totalRooms + 
                      totalBedrooms + population + households + medianIncome, 
                    data=CAhousing_train, distribution = "gaussian", n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4, cv.folds = 5)

CAhousing_gbm_rmse = data.frame(
CAhousing_gbm1$cv.error %>% mean %>% sqrt,
CAhousing_gbm2$cv.error %>% mean %>% sqrt
)
colnames(CAhousing_gbm_rmse) = c("Boosted 1", "Boosted 2")
rownames(CAhousing_gbm_rmse) = "RMSE"
kable(CAhousing_gbm_rmse)
```

We compare the out-of-sample RMSE's of the best tree, random forest, and gradient boosted models along with the baseline linear model and the stepwise-selected model.

```{r}
CAhousing_rmse = data.frame(
modelr::rmse(CAhousing_lm, CAhousing_test),
modelr::rmse(CAhousing_step, CAhousing_test),
modelr::rmse(CAhousing.tree, CAhousing_test),
modelr::rmse(CAhousing.prune, CAhousing_test),
modelr::rmse(CAhousing.forest, CAhousing_test),
modelr::rmse(CAhousing_gbm1, CAhousing_test))
colnames(CAhousing_rmse) = c("Linear", "Stepwise", "Tree", "Pruned Tree", "Forest", "GBM")
rownames(CAhousing_rmse) = "RMSE"
kable(CAhousing_rmse)
#proceed with GBM model
```

Therefore, we choose the gradient boosted model that has all the variables as the features. 
This makes sense, as the price would be affected by the location of the house. 
The plots are below.

```{r}
CAmap = get_map("california", zoom=6)
```

```{r}
ggmap(CAmap) + 
  geom_point(aes(x = longitude, y = latitude, color = medianHouseValue/100000), data = CAhousing, alpha = .4) +
  labs(x = 'Longitude', y = 'Latitude', title = "Median House Values in California", 
       color = 'in 100,000 Dollars')
```

```{r}
CApredict = CAhousing %>% mutate(predictedMedianHouseValue = predict.gbm(CAhousing_gbm1, newdata = CAhousing))
ggmap(CAmap) + 
  geom_point(aes(x = longitude, y = latitude, color = predictedMedianHouseValue/100000), data = CApredict, alpha = .4) +
  labs(x = 'Longitude', y = 'Latitude', title = "Predicted Median House Values in California", 
       color = 'in 100,000 Dollars')
```

```{r}
CApredict = CApredict %>% mutate(residuals = medianHouseValue - predictedMedianHouseValue)
ggmap(CAmap) + 
  geom_point(aes(x = longitude, y = latitude, color = residuals/100000), data = CApredict, alpha = .4) +
  labs(x = 'Longitude', y = 'Latitude', title = "Residuals of the Predictions",
       color = 'in 100,000 Dollars')
```

