---
title: "ECO395M: Exercise 4"
author: "Steven Kim and Shreekara Shastry"
date: ""
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(knitr)
library(ggfortify)
library(foreach)
library(arules)
library(arulesViz)
library(factoextra)
library(ggpubr)
library(igraph)
```

## Clustering and PCA

We first ran PCA on those 11 chemical properties.

```{r winedata}
set.seed(389)
#reading the data and initial treatment
wine = read.csv("wine.csv")
wine$color = wine$color %>% as.factor
wine$quality = wine$quality %>% as.factor
```

```{r winepca}
wine1 = wine[,1:11]
PCAwine = prcomp(wine1, scale=TRUE, rank=3)

#plot(PCAwine)
#summary(PCAwine)
#round(PCAwine$rotation[,1:3],3) 

autoplot(PCAwine, data = wine, colour = 'color') +
  labs(title = "Graph of PC1 and PC2, by colors of wine")
autoplot(PCAwine, data = wine, colour = 'quality', alpha = .9, 
         loadings = TRUE, loadings.colour = 'pink',
         loadings.label = TRUE, loadings.label.size = 3) +
  labs(title = "Graph of PC1 and PC2, by qualit ratings of wine")
```

From the first graph, we can see that red and white wines sort of form clusters. 
This suggests that even after applying PCA, clustering would be useful.
The second graph says that the qualities are kind of all over places, not easy to find a pattern.

```{r wineclustering}
wine2 = scale(wine[,1:11])

# cluster on measurables k_grid = seq(2, 30, by=1)
# N = nrow(wine2)
# k_grid = seq(2, 30, by=1)
# SSE_grid = foreach(k = k_grid, .combine='rbind') %do% {
# cluster_k = kmeans(wine2, k, nstart=50)
# W = cluster_k$tot.withinss
# B = cluster_k$betweenss
# CH = (B/W)*((N-k)/(k-1))
# c(k=k, CH=CH)} %>% as.data.frame
# ggplot(SSE_grid) +
#   geom_line(aes(x=k, y=CH))
#choose k=10
# Clusterwine = kmeans(wine2, 10, nstart=50)

wineclustered = kmeans(wine2, 2, nstart=50)
qplot(quality, color, data=wine, color=factor(wineclustered$cluster)) +
  labs(title = "Graph of clusters by each quality rating and color of wine",
        color = "cluster")

wineclustered$color = wine$color
# fviz_cluster(wineclustered, data = wine2,
#              palette = c("#2E9FDF", "#00AFBB"), 
#              geom = "point", shape = 0,
#              ellipse.type = "convex", 
#              ggtheme = theme_bw()
#              ) + geom_point(aes(shape = wineclustered$color), alpha = 0.5)

wineclustered2 = kmeans(wine2, 4, nstart=50)
qplot(quality, color, data=wine, color=factor(wineclustered2$cluster)) +
  labs(title = "Graph of clusters by each quality rating and color of wine",
        color = "cluster")

# wineclustered2$quality = wine$quality %>% as.factor
# fviz_cluster(wineclustered2, data = wine2,
#              geom = "point", shape = 0,
#              ellipse.type = "convex", 
#              ggtheme = theme_bw()
#              )
```

I first picked the model of 2 clusters to see if it would naturally form two clusters of red and white wines. 
The first graph represents how the two clusters are formed with the color and quality, it looks like one cluster has red wine 
and the other has white wine, across all qualities. 

Next, I picked the model of 4 clusters, hoping to see four groups of wine, low/high qualities and red/wine. 
The second graph represents the 4 clusters across different colors and qualities of wine. 
I can't really find a set pattern for the groups I was hoping to find. 

I then went ahead to graph the clusters on the PCA axes, having the shape denote the color/quality. 
We can see how useful the clusters are to explain the colors but not so much for the qualities. 
That is, almost all points belong to one color of wine in the first graph, whereas there are many points 
that are in different clusters with the same quality.


```{r wineclustering-graph1}
## Using PCA to graph clusters
#1. colors - two clusters
# Coordinates of individuals
wine.coord <- as.data.frame(get_pca_ind(PCAwine)$coord)
# Add clusters obtained using the K-means algorithm
wine.coord$cluster <- factor(wineclustered$cluster)
# Add Species groups from the original data sett
wine.coord$color <- wine$color

# Percentage of variance explained by dimensions
eigenvalue <- round(get_eigenvalue(PCAwine), 1)
variance.percent <- eigenvalue$variance.percent

ggscatter(
  wine.coord, x = "Dim.1", y = "Dim.2", 
  color = "cluster", palette = "npg", ellipse = TRUE, ellipse.type = "convex",
  shape = "color", size = 1.5,  legend = "right", ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent[1], "% )" ),
  ylab = paste0("Dim 2 (", variance.percent[2], "% )" )
) +
  labs(title = "Graph of clusters on PCA axes by color of wine")

```

```{r wineclustering-graph2}
#2. qualities - seven clusters
# Coordinates of individuals
wine.coord <- as.data.frame(get_pca_ind(PCAwine)$coord)
# Add clusters obtained using the K-means algorithm
wine.coord$cluster <- factor(wineclustered2$cluster)
# Add Species groups from the original data sett
wine.coord$quality <- wine$quality

# Percentage of variance explained by dimensions
eigenvalue <- round(get_eigenvalue(PCAwine), 1)
variance.percent <- eigenvalue$variance.percent

ggscatter(
  wine.coord, x = "Dim.1", y = "Dim.2", 
  color = "cluster", palette = "npg", ellipse = TRUE, ellipse.type = "convex",
  shape = "quality", size = 1.5,  legend = "right", ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent[1], "% )" ),
  ylab = paste0("Dim 2 (", variance.percent[2], "% )" )
) +
  labs(title = "Graph of clusters on PCA axes by each quality rating")
```



## Market Segmentation

```{r market_segmentation, results = 'hide'}
social = read.csv("social_marketing.csv") %>% drop_na()
```

Let's get some useful information about the market. In this code snippet we are
checking which are the top 5 popular tweet categories.
```{r top5tweets}
# Let's see which category has the most amount of tweets
colTotals = colSums(social[,-1], na.rm = TRUE) %>% as.data.frame()
names(colTotals) = c('Total')

kable(colTotals %>% arrange(desc(Total)) %>% head())
```

Ignoring chatter since it doesn't belong to any single category. `Photo sharing`,
`health_nutrition`, `cooking`,  `politics`, `sports_fandom` are the top 4 popular
tweet categories in the dataset.

Let's try to run a standard PCA algoritm on the data.

```{r pca, results='hide'}
# PCA
socialPCA = prcomp(social[,-1], scale=TRUE, rank=3)
autoplot(socialPCA) +
  labs(title = "PCA graph on Market Segments")
summary(socialPCA)
```

The standard PCA doesn't really help in this case because we are not looking into a specific 
category of tweet. But grouping them into clusters along with this graph can show us
the categories that are grouped together.

Let's do a hierarchical clustering model and see if we can group the categories together
to get a more idea about the broader market segments.

```{r hcluster, results='hide'}
# Clustering hclust
social_scaled = scale(social[,-1], center = TRUE, scale = TRUE)
social_distance_matrix = dist(social_scaled, method='euclidean')

social_clust = hclust(social_distance_matrix, method="average")
plot(social_clust, cex=0.8)

social_cluster = cutree(social_clust, k=2)
summary(factor(social_cluster))
```

The cluster dendogram which uses the euclidean distance and the average distance to group
doesn't give us any information about the marget segments in general.

Now let's perform k-means clustering which I think coupled with the PCA methods will give
a meaningful market segments. First step is to find the optimal value for k that
we should use in the `kmeans` method. For each value of `k`, calculate the `withiness`,
`betweeness` and `CH-Index` from these values. This can help us get the optimal value of `k`.

```{r clusterBtwness}
# k-means clustering
k_grid = seq(1, 11, by=1)
SSE_grid = foreach(k = k_grid, .combine='rbind') %do% {
cluster_k = kmeans(social[,-1], k, nstart=50)
W = cluster_k$tot.withinss
B = cluster_k$betweenss
CH = (B/W)*((nrow(social[,-1])-k)/(k-1))
c(k=k, CH=CH)} %>% as.data.frame
ggplot(SSE_grid) +
  geom_line(aes(x=k, y=CH)) +
  labs(title = "CH-Index change over different values of k")
```

We decided to go with `k=3`. I think the improvement stops after `k=3`.

```{r kmeans_cluster}
socialCluster = kmeans(social[,-1], centers = 3, nstart=50)
fviz_cluster(socialCluster, social[,-1],
             palette = c("#2E9FDF", "#00AFBB", "#C8E1AB"),
             geom = "point",
             ellipse.type = "convex",
             ggtheme = theme_bw()
             )
```

This plot shows how the clusters divide the PCA plot and looks like each of these 
graphs have different categories. let's analyze more on that. We start by selecting 
the k-means cluster and grouping them by the cluster and calculate the sum of the 
number of tweets. Also remove the `cluster`, `chatter`, and `uncategorized` as they
are not a part of any specific market segment. 

```{r kmeans_analysis}
socialClust = rename(merge(social[,-1], socialCluster$cluster, by="row.names"), cluster = y)

socialClusterTable = socialClust %>%
  group_by(cluster) %>%
  select(-Row.names) %>%
  summarize_all(sum) %>%
  select(-c(cluster, chatter, uncategorized))

socialClusterTable = t(as.data.frame(socialClusterTable))

socialClusterTable = tibble::rownames_to_column(socialClusterTable %>% as.data.frame())
```

Let's plot the top 5 categories suggested by each of the clusters used in k-means.

```{r top5_eachCluster, results='hide'}
names(socialClusterTable) = c("Category", "Cluster1", "Cluster2", "Cluster3")

socialClusterTable %>% mutate(Cluster1= round(Cluster1/sum(Cluster1)*100,2)) %>% top_n(n=10, Cluster1) %>% ggplot() +
  geom_col(aes(x=reorder(Category,-Cluster1), y=Cluster1 ), fill = "#2E9FDF") +
  labs(y = "Percentage", x="Tweet Categories", 
       title = "Top 10 categories within Cluster 1 by the proportion of tweets") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

socialClusterTable %>% mutate(Cluster2= round(Cluster2/sum(Cluster2)*100,2)) %>% top_n(n=10, Cluster2) %>% ggplot() + 
  geom_col(aes(x=reorder(Category,-Cluster2), y=Cluster2 ), fill = "#00AFBB") +
  labs(y = "Percentage", x="Tweet Categories", 
       title = "Top 10 categories within Cluster 2 by the proportion of tweets") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

socialClusterTable %>% mutate(Cluster2= round(Cluster2/sum(Cluster2)*100,2)) %>% top_n(n=10, Cluster3) %>% ggplot() + 
  geom_col(aes(x=reorder(Category,-Cluster3), y=Cluster3 ), fill = "#C8E1AB") +
  labs(y = "Percentage", x="Tweet Categories", 
       title = "Top 10 categories within Cluster 3 by the proportion of tweets") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Each clusters give us interesting results,

* `Cluster1` - All the top 10 categories shown in the graph share similar percentage of the number of tweets between 4-6%. This category looks like representative of topics related to or covered in traditional mass media.
* `Cluster2` - `photo_sharing`, `cooking`, `shopping`, and `college_uni` are the top 4 categories from this market category.
I think this is representative of the young adults and college students because people generally associate those activities with the young adults.
* `Cluster3` - `health_nutrition`, `personal_fitness` and `cooking` form separate market segment. This looks like a separate market segment where `health` is important.

We also see a lot of overlaps in the tweet categories between the different clusters which is expected but `k-means` manages to extract some useful information about the "market categories" of people using the `NutrientH20` product. Based on this we suggest that they target three distinct groups of **General Public / Traditional Mass Media / Current Politics and News**, **Young Adults / **, and **Personal Fitness / Healthy Lifestyles**.

## Association rules for grocery purchase

```{r groceries-1, results='hide'}
groceries_raw = readLines('groceries.txt')
groceries = as.list(strsplit(groceries_raw, ","))
groceries = lapply(groceries, unique)

#average number of items in groceries is 4.41
matrix(data= c(lengths(groceries)),
           ncol = 1,
           byrow = TRUE) %>% mean

## Cast this resulting list of playlists as a special arules "transactions" class.
grocerytrans = as(groceries, "transactions")
#summary(grocerytrans)

# Now run the 'apriori' algorithm
# Look at rules with support > .01 & confidence >.1 & length (# artists) <= 5
groceryrules = apriori(grocerytrans, 
	parameter=list(support=.002, confidence=.4, maxlen=4))
                         
# Look at the output... so many rules!
# inspect(groceryrules)
# summary(groceryrules)
```

As the average number of items in groceries list is 4.41, I chose the `maxlen` to be 4. 
Also, since there are so many items, I chose a low support of .002 as the threshold, with confidence equal to .4. 
With higher support I did not get many rules. 
Plus, I thought if someone buys something after putting another thing in the shopping cart with probability of 0.4, it's pretty meaningful.
The table below shows the rules with lift higher than 4 out of all the rules with the parameters I specified.

```{r groceries-kable}
## Choose a subset
kable(inspect(subset(groceryrules, lift > 4)))
```

It's interesting to see that if a person buys `liquor`, that person is likely to buy `bottled beer` as well, which makes a lot of sense. 
There are many rules with `root vegetables` on the right hand side.

```{r groceries-2}
# plot all the rules in (support, confidence) space
# notice that high lift rules tend to have low support
plot(groceryrules)

# can swap the axes and color scales
plot(groceryrules, measure = c("support", "lift"), shading = "confidence")

# "two key" plot: coloring is by size (order) of item set ???
plot(groceryrules, method='two-key plot')

# can now look at subsets driven by the plot
#inspect(subset(groceryrules, support > 0.04))
#inspect(subset(groceryrules, confidence > 0.4))
```

From these three graphs, we can see that the majority of rules have support less than 0.01. 
Another interesting fact is that a lot of rules have the order of three or four. 
They tend to have lower support as well, which is probably because it's not likely to have that specific 
combination in a shopping cart.

I then plotted top 70 rules by lift. 

```{r groceries-3}
# graph-based visualization
plot(head(groceryrules, 70, by='lift'), method="graph", control=list(type="itemsets"))
```

We can see the highest lift rule of `liquor` &rarr; `bottled beer`. 
`root vegetables` and `other vegetables` have very big circles.

The graph below shows top 30 rules by lift.

```{r groceries-4}
plot(head(groceryrules, 30, by='lift'),method="graph")
```


