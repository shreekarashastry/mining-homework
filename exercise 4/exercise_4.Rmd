---
title: "ECO395M: Exercise 4"
author: "Steven Kim and Shreekara Shastry"
date: ""
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(knitr)
library(ggfortify)
library(foreach)
library(arules)
library(arulesViz)
library(factoextra)
library(ggpubr)
```

## Clustering and PCA

We first ran PCA on those 11 chemical properties.

```{r winedata}
#reading the data and initial treatment
wine = read.csv("wine.csv")
wine$color = wine$color %>% as.factor
wine$quality = wine$quality %>% as.factor
```

```{r winepca}
wine1 = wine[,1:11]
PCAwine = prcomp(wine1, scale=TRUE, rank=3)

#plot(PCAwine)
#summary(PCAwine)
#round(PCAwine$rotation[,1:3],3) 

autoplot(PCAwine, data = wine, colour = 'color')
autoplot(PCAwine, data = wine, colour = 'quality', alpha = .9, 
         loadings = TRUE, loadings.colour = 'pink',
         loadings.label = TRUE, loadings.label.size = 3)
```

From the first graph, we can see that red and white wines have pretty different principal components. 
That is, we can easily distinguish

```{r wineclustering}
wine2 = scale(wine[,1:11])

# cluster on measurables k_grid = seq(2, 30, by=1)
# N = nrow(wine2)
# k_grid = seq(2, 30, by=1)
# SSE_grid = foreach(k = k_grid, .combine='rbind') %do% {
# cluster_k = kmeans(wine2, k, nstart=50)
# W = cluster_k$tot.withinss
# B = cluster_k$betweenss
# CH = (B/W)*((N-k)/(k-1))
# c(k=k, CH=CH)} %>% as.data.frame
# ggplot(SSE_grid) +
#   geom_line(aes(x=k, y=CH))
#choose k=10
# Clusterwine = kmeans(wine2, 10, nstart=50)

wineclustered = kmeans(wine2, 2, nstart=50)
qplot(quality, color, data=wine, color=factor(wineclustered$cluster))

wineclustered$color = wine$color
# fviz_cluster(wineclustered, data = wine2,
#              palette = c("#2E9FDF", "#00AFBB"), 
#              geom = "point", shape = 0,
#              ellipse.type = "convex", 
#              ggtheme = theme_bw()
#              ) + geom_point(aes(shape = wineclustered$color), alpha = 0.5)

wineclustered2 = kmeans(wine2, 7, nstart=50)
qplot(quality, color, data=wine, color=factor(wineclustered2$cluster))

# wineclustered2$quality = wine$quality %>% as.factor
# fviz_cluster(wineclustered2, data = wine2,
#              geom = "point", shape = 0,
#              ellipse.type = "convex", 
#              ggtheme = theme_bw()
#              )
```

```{r wineclustering-graph1}
## Using PCA to graph clusters
#1. colors - two clusters
# Coordinates of individuals
wine.coord <- as.data.frame(get_pca_ind(PCAwine)$coord)
# Add clusters obtained using the K-means algorithm
wine.coord$cluster <- factor(wineclustered$cluster)
# Add Species groups from the original data sett
wine.coord$color <- wine$color

# Percentage of variance explained by dimensions
eigenvalue <- round(get_eigenvalue(PCAwine), 1)
variance.percent <- eigenvalue$variance.percent

ggscatter(
  wine.coord, x = "Dim.1", y = "Dim.2", 
  color = "cluster", palette = "npg", ellipse = TRUE, ellipse.type = "convex",
  shape = "color", size = 1.5,  legend = "right", ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent[1], "% )" ),
  ylab = paste0("Dim 2 (", variance.percent[2], "% )" )
)

```

```{r wineclustering-graph2}
#2. qualities - seven clusters
# Coordinates of individuals
wine.coord <- as.data.frame(get_pca_ind(PCAwine)$coord)
# Add clusters obtained using the K-means algorithm
wine.coord$cluster <- factor(wineclustered2$cluster)
# Add Species groups from the original data sett
wine.coord$quality <- wine$quality

# Percentage of variance explained by dimensions
eigenvalue <- round(get_eigenvalue(PCAwine), 1)
variance.percent <- eigenvalue$variance.percent

ggscatter(
  wine.coord, x = "Dim.1", y = "Dim.2", 
  color = "cluster", palette = "npg", ellipse = TRUE, ellipse.type = "convex",
  shape = "quality", size = 1.5,  legend = "right", ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent[1], "% )" ),
  ylab = paste0("Dim 2 (", variance.percent[2], "% )" )
)
```

## Market Segmentation

```{r market_segmentation}
social = read.csv("social_marketing.csv") %>% drop_na()

# Let's see which category has the most amount of tweets
colTotals = colSums(social[,-1], na.rm = TRUE) %>% as.data.frame()
names(colTotals) = c('Total')

colTotals %>% arrange(desc(Total)) %>% head()

# PCA - Meep
socialPCA = prcomp(social[,-1], scale=TRUE, rank=3)
autoplot(socialPCA)
summary(socialPCA)

# Clustering hclust - Meep
social_scaled = scale(social[,-1], center = TRUE, scale = TRUE)
social_distance_matrix = dist(social_scaled, method='euclidean')

social_clust = hclust(social_distance_matrix, method="average")
plot(social_clust, cex=0.8)

social_cluster = cutree(social_clust, k=2)
summary(factor(social_cluster))

# k-means clustering
social_top10 = head(sort(colSums(social[,-1]), decreasing = TRUE), 11)
social_top10 = subset(social[,-1][, names(social_top10)], select = -chatter)

k_grid = seq(1, 11, by=1)
SSE_grid = foreach(k = k_grid, .combine='rbind') %do% {
cluster_k = kmeans(social[,-1], k, nstart=50)
W = cluster_k$tot.withinss
B = cluster_k$betweenss
CH = (B/W)*((nrow(social[,-1])-k)/(k-1))
c(k=k, CH=CH)} %>% as.data.frame
ggplot(SSE_grid) +
  geom_line(aes(x=k, y=CH))

socialCluster = kmeans(social[,-1], centers = 3, nstart=50)
fviz_cluster(socialCluster, social[,-1])

socialClust = rename(merge(social[,-1], socialCluster$cluster, by="row.names"), cluster = y)

socialClusterTable = socialClust %>%
  group_by(cluster) %>%
  select(-Row.names) %>%
  summarize_all(sum) %>%
  select(-c(cluster, chatter, uncategorized))

socialClusterTable = t(as.data.frame(socialClusterTable))

socialClusterTable = tibble::rownames_to_column(socialClusterTable %>% as.data.frame())

names(socialClusterTable) = c("Category", "Cluster1", "Cluster2", "Cluster3")

top_n(socialClusterTable, n=5, Cluster1) %>% ggplot() + 
  geom_col(aes(x=Category, y=Cluster1 )) + coord_flip()

top_n(socialClusterTable, n=5, Cluster2) %>% ggplot() + 
  geom_col(aes(x=Category, y=Cluster2 )) + coord_flip()

top_n(socialClusterTable, n=5, Cluster3) %>% ggplot() + 
  geom_col(aes(x=Category, y=Cluster3 )) + coord_flip()
```

## Association rules for grocery purchase

```{r groceries}
groceries_raw = readLines('groceries.txt')
groceries = as.list(strsplit(groceries_raw, ","))
groceries = lapply(groceries, unique)


## Cast this resulting list of playlists as a special arules "transactions" class.
grocerytrans = as(groceries, "transactions")
#summary(grocerytrans)

# Now run the 'apriori' algorithm
# Look at rules with support > .01 & confidence >.1 & length (# artists) <= 5
groceryrules = apriori(grocerytrans, 
	parameter=list(support=.01, confidence=.1, maxlen=2))
                         
# Look at the output... so many rules!
#inspect(groceryrules)

#summary(groceryrules)

## Choose a subset
#inspect(subset(groceryrules, lift > 2 & confidence > 0.35))

### started from values near the max, and lowered to have around 5.

# plot all the rules in (support, confidence) space
# notice that high lift rules tend to have low support
plot(groceryrules)

# can swap the axes and color scales
plot(groceryrules, measure = c("support", "lift"), shading = "confidence")

# "two key" plot: coloring is by size (order) of item set ???
plot(groceryrules, method='two-key plot')

# can now look at subsets driven by the plot
#inspect(subset(groceryrules, support > 0.04))
#inspect(subset(groceryrules, confidence > 0.4))

# graph-based visualization
sub1 = subset(groceryrules, subset=confidence > 0.01 & support > 0.005)
summary(sub1)
plot(head(sub1, 100, by='lift'), method='graph')
```

Chose the confidence and the support cutoff levels so that the subset have around 25 rows.
