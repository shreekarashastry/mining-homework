---
title: "ECO395M: Exercise 4"
author: "Steven Kim and Shreekara Shastry"
date: ""
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(knitr)
library(ggfortify)
library(foreach)
library(arules)
library(arulesViz)
library(factoextra)
library(ggpubr)
```

## Clustering and PCA

We first ran PCA on those 11 chemical properties.

```{r winedata}
set.seed(389)
#reading the data and initial treatment
wine = read.csv("wine.csv")
wine$color = wine$color %>% as.factor
wine$quality = wine$quality %>% as.factor
```

```{r winepca}
wine1 = wine[,1:11]
PCAwine = prcomp(wine1, scale=TRUE, rank=3)

#plot(PCAwine)
#summary(PCAwine)
#round(PCAwine$rotation[,1:3],3) 

autoplot(PCAwine, data = wine, colour = 'color')
autoplot(PCAwine, data = wine, colour = 'quality', alpha = .9, 
         loadings = TRUE, loadings.colour = 'pink',
         loadings.label = TRUE, loadings.label.size = 3)
```

From the first graph, we can see that red and white wines sort of form clusters. 
This suggests that even after applying PCA, clustering would be useful.
The second graph says that the qualities are kind of all over places, not easy to find a pattern.

```{r wineclustering}
wine2 = scale(wine[,1:11])

# cluster on measurables k_grid = seq(2, 30, by=1)
# N = nrow(wine2)
# k_grid = seq(2, 30, by=1)
# SSE_grid = foreach(k = k_grid, .combine='rbind') %do% {
# cluster_k = kmeans(wine2, k, nstart=50)
# W = cluster_k$tot.withinss
# B = cluster_k$betweenss
# CH = (B/W)*((N-k)/(k-1))
# c(k=k, CH=CH)} %>% as.data.frame
# ggplot(SSE_grid) +
#   geom_line(aes(x=k, y=CH))
#choose k=10
# Clusterwine = kmeans(wine2, 10, nstart=50)

wineclustered = kmeans(wine2, 2, nstart=50)
qplot(quality, color, data=wine, color=factor(wineclustered$cluster))

wineclustered$color = wine$color
# fviz_cluster(wineclustered, data = wine2,
#              palette = c("#2E9FDF", "#00AFBB"), 
#              geom = "point", shape = 0,
#              ellipse.type = "convex", 
#              ggtheme = theme_bw()
#              ) + geom_point(aes(shape = wineclustered$color), alpha = 0.5)

wineclustered2 = kmeans(wine2, 4, nstart=50)
qplot(quality, color, data=wine, color=factor(wineclustered2$cluster))

# wineclustered2$quality = wine$quality %>% as.factor
# fviz_cluster(wineclustered2, data = wine2,
#              geom = "point", shape = 0,
#              ellipse.type = "convex", 
#              ggtheme = theme_bw()
#              )
```

I first picked the model of 2 clusters to see if it would naturally form two clusters of red and white wines. 
The first graph represents how the two clusters are formed with the color and quality, it looks like one cluster has red wine 
and the other has white wine, across all qualities. 

Next, I picked the model of 4 clusters, hoping to see four groups of wine, low/high qualities and red/wine. 
The second graph represents the 4 clusters across different colors and qualities of wine. 
I can't really find a set pattern for the groups I was hoping to find. 

I then went ahead to graph the clusters on the PCA axes, having the shape denote the color/quality. 
We can see how useful the clusters are to explain the colors but not so much for the qualities. 
That is, almost all points belong to one color of wine in the first graph, whereas there are many points 
that are in different clusters with the same quality.


```{r wineclustering-graph1}
## Using PCA to graph clusters
#1. colors - two clusters
# Coordinates of individuals
wine.coord <- as.data.frame(get_pca_ind(PCAwine)$coord)
# Add clusters obtained using the K-means algorithm
wine.coord$cluster <- factor(wineclustered$cluster)
# Add Species groups from the original data sett
wine.coord$color <- wine$color

# Percentage of variance explained by dimensions
eigenvalue <- round(get_eigenvalue(PCAwine), 1)
variance.percent <- eigenvalue$variance.percent

ggscatter(
  wine.coord, x = "Dim.1", y = "Dim.2", 
  color = "cluster", palette = "npg", ellipse = TRUE, ellipse.type = "convex",
  shape = "color", size = 1.5,  legend = "right", ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent[1], "% )" ),
  ylab = paste0("Dim 2 (", variance.percent[2], "% )" )
)

```

```{r wineclustering-graph2}
#2. qualities - seven clusters
# Coordinates of individuals
wine.coord <- as.data.frame(get_pca_ind(PCAwine)$coord)
# Add clusters obtained using the K-means algorithm
wine.coord$cluster <- factor(wineclustered2$cluster)
# Add Species groups from the original data sett
wine.coord$quality <- wine$quality

# Percentage of variance explained by dimensions
eigenvalue <- round(get_eigenvalue(PCAwine), 1)
variance.percent <- eigenvalue$variance.percent

ggscatter(
  wine.coord, x = "Dim.1", y = "Dim.2", 
  color = "cluster", palette = "npg", ellipse = TRUE, ellipse.type = "convex",
  shape = "quality", size = 1.5,  legend = "right", ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent[1], "% )" ),
  ylab = paste0("Dim 2 (", variance.percent[2], "% )" )
)
```



## Market Segmentation

```{r market_segmentation, results = 'hide'}
social = read.csv("social_marketing.csv") %>% drop_na()

# Let's see which category has the most amount of tweets
colTotals = colSums(social[,-1], na.rm = TRUE) %>% as.data.frame()
names(colTotals) = c('Total')

colTotals %>% arrange(desc(Total)) %>% head()

# PCA - Meep
socialPCA = prcomp(social[,-1], scale=TRUE, rank=3)
autoplot(socialPCA)
summary(socialPCA)

# Clustering hclust - Meep
social_scaled = scale(social[,-1], center = TRUE, scale = TRUE)
social_distance_matrix = dist(social_scaled, method='euclidean')

social_clust = hclust(social_distance_matrix, method="average")
plot(social_clust, cex=0.8)

social_cluster = cutree(social_clust, k=2)
summary(factor(social_cluster))

# k-means clustering
social_top10 = head(sort(colSums(social[,-1]), decreasing = TRUE), 11)
social_top10 = subset(social[,-1][, names(social_top10)], select = -chatter)

k_grid = seq(1, 11, by=1)
SSE_grid = foreach(k = k_grid, .combine='rbind') %do% {
cluster_k = kmeans(social[,-1], k, nstart=50)
W = cluster_k$tot.withinss
B = cluster_k$betweenss
CH = (B/W)*((nrow(social[,-1])-k)/(k-1))
c(k=k, CH=CH)} %>% as.data.frame
ggplot(SSE_grid) +
  geom_line(aes(x=k, y=CH))

socialCluster = kmeans(social[,-1], centers = 3, nstart=50)
fviz_cluster(socialCluster, social[,-1])

socialClust = rename(merge(social[,-1], socialCluster$cluster, by="row.names"), cluster = y)

socialClusterTable = socialClust %>%
  group_by(cluster) %>%
  select(-Row.names) %>%
  summarize_all(sum) %>%
  select(-c(cluster, chatter, uncategorized))

socialClusterTable = t(as.data.frame(socialClusterTable))

socialClusterTable = tibble::rownames_to_column(socialClusterTable %>% as.data.frame())

names(socialClusterTable) = c("Category", "Cluster1", "Cluster2", "Cluster3")

top_n(socialClusterTable, n=5, Cluster1) %>% ggplot() + 
  geom_col(aes(x=Category, y=Cluster1 )) + coord_flip()

top_n(socialClusterTable, n=5, Cluster2) %>% ggplot() + 
  geom_col(aes(x=Category, y=Cluster2 )) + coord_flip()

top_n(socialClusterTable, n=5, Cluster3) %>% ggplot() + 
  geom_col(aes(x=Category, y=Cluster3 )) + coord_flip()
```

## Association rules for grocery purchase

```{r groceries, results = 'hide'}
groceries_raw = readLines('groceries.txt')
groceries = as.list(strsplit(groceries_raw, ","))
groceries = lapply(groceries, unique)


## Cast this resulting list of playlists as a special arules "transactions" class.
grocerytrans = as(groceries, "transactions")
#summary(grocerytrans)

# Now run the 'apriori' algorithm
# Look at rules with support > .01 & confidence >.1 & length (# artists) <= 5
groceryrules = apriori(grocerytrans, 
	parameter=list(support=.001, confidence=.7, maxlen=10))
                         
# Look at the output... so many rules!
#inspect(groceryrules)

summary(groceryrules)

## Choose a subset
inspect(subset(groceryrules, lift > 2 & confidence > 0.35))

### started from values near the max, and lowered to have around 5.

# plot all the rules in (support, confidence) space
# notice that high lift rules tend to have low support
plot(groceryrules)

# can swap the axes and color scales
plot(groceryrules, measure = c("support", "lift"), shading = "confidence")

# "two key" plot: coloring is by size (order) of item set ???
plot(groceryrules, method='two-key plot')

# can now look at subsets driven by the plot
inspect(subset(groceryrules, support > 0.04))
inspect(subset(groceryrules, confidence > 0.4))

# graph-based visualization
sub1 = subset(groceryrules, subset=confidence > 0.01 & support > 0.005)
summary(sub1)
plot(head(sub1, 100, by='lift'), method='graph')
saveAsGraph(sub1, file = "groceryrules.graphml")
```

Chose the confidence and the support cutoff levels so that the subset have around 25 rows.
